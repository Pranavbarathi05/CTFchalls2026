# The Robots Are Watching

A beginner-friendly web challenge teaching about robots.txt and web crawling directives.

## Challenge Info

| Field | Value |
|-------|-------|
| **Name** | The Robots Are Watching |
| **Category** | Web Exploitation |
| **Difficulty** | Easy |
| **Points** | 100 |
| **Flag** | `DSCCTF{r0b0ts_txt_1s_n0t_s3cur1ty_2026}` |
| **Port** | 5001 |

---

## Description

ü§ñ The Robots Are Watching [web] - 100 pts

Welcome to TechCorp Industries, where we build the future of automation!

Our website is constantly monitored by our robotic systems.
They follow certain protocols... certain rules...

The robots are always watching.
Perhaps they're watching something they shouldn't?

üîç Can you discover what the robots are guarding?

---

## Solution

1. Visit the main page at `http://localhost:5001/`
2. Notice the hint: "The robots are always watching... They follow certain rules"
3. Check `/robots.txt` - a standard file that tells web crawlers what to index
4. See the disallowed paths, especially `/secret-admin-panel/`
5. Visit `http://localhost:5001/secret-admin-panel/`
6. Get the flag: `DSCCTF{r0b0ts_txt_1s_n0t_s3cur1ty_2026}`

---

## Learning Objectives

- **robots.txt**: Understanding the standard for web crawler directives
- **Security by obscurity**: robots.txt is NOT a security measure
- **Reconnaissance**: Using standard files for web application discovery
- **Information disclosure**: Sensitive paths listed in robots.txt

---

## Deployment

```bash
cd robots-watching
docker-compose up -d --build
```

Access at: `http://localhost:5001`

---

## Key Concepts

### What is robots.txt?

The `robots.txt` file is a standard used by websites to communicate with web crawlers and search engines. It tells automated bots which parts of the site they should or shouldn't crawl.

**Important:** It's NOT a security mechanism! It only provides guidance to well-behaved bots.

### Common Mistake

Developers sometimes list sensitive paths in robots.txt thinking it will hide them. Instead, it actually advertises these paths to attackers!

### Real-World Impact

Many real websites have accidentally exposed:
- Admin panels
- Backup directories
- Development/staging areas
- API endpoints
- Internal documentation

---

## Testing

```bash
# Check robots.txt
curl http://localhost:5001/robots.txt

# Access the secret panel
curl http://localhost:5001/secret-admin-panel/
```

---

## Variations for Difficulty

**Make Easier:**
- More obvious hints in the main page
- Larger font for the hint text
- Add "Check robots.txt" directly

**Make Harder:**
- Multiple fake paths in robots.txt
- Hidden comment in HTML source hinting at robots.txt
- Add other standard files like sitemap.xml

---

## Files

```
robots-watching/
‚îú‚îÄ‚îÄ app.py              # Flask application
‚îú‚îÄ‚îÄ Dockerfile          # Container setup
‚îú‚îÄ‚îÄ docker-compose.yml  # Easy deployment
‚îú‚îÄ‚îÄ description.md      # Challenge description
‚îî‚îÄ‚îÄ README.md          # This file
```

---

## Author Notes

This challenge teaches one of the first things security researchers check when assessing a web application. The robots.txt file often contains valuable information about site structure and potentially sensitive areas.

The hint "robots are always watching" combined with the robot emoji should guide players toward checking robots.txt.

---

## Flag

`DSCCTF{r0b0ts_txt_1s_n0t_s3cur1ty_2026}`
